# üöÄ MAIA Deployment Plan - Keep Current Voices

**Goal**: Deploy everything we built, keep using OpenAI's Alloy/Shimmer voices for now, research custom voices later.

---

## Phase 1: Conversation UI (Deploy NOW - 30 mins)

### What This Gives Users:
- ‚úÖ Review all past conversations
- ‚úÖ Search and filter
- ‚úÖ Download transcripts
- ‚úÖ Extract conversation essence
- ‚úÖ Beautiful organized view

### Deployment Steps:

**Step 1: Verify Files (2 mins)**
```bash
cd /Users/soullab/MAIA-FRESH

# These files should exist:
ls components/EnhancedConversationReview.tsx
ls app/conversations/page.tsx

# If they exist, you're good!
```

**Step 2: Add to Navigation (5 mins)**

Find your main navigation file. Likely one of these:
- `components/Navigation.tsx`
- `components/Sidebar.tsx`
- `app/layout.tsx`

Add this link:
```typescript
import { MessageCircle } from 'lucide-react';

// Wherever your nav links are:
<Link href="/conversations" className="nav-link">
  <MessageCircle className="w-5 h-5" />
  <span>Conversations</span>
</Link>
```

**Step 3: Test Locally (3 mins)**
```bash
npm run dev

# Open browser
open http://localhost:3000/conversations

# You should see the conversation review page!
```

**Step 4: Deploy to Vercel (5 mins)**
```bash
git add .
git commit -m "Add conversation review UI"
git push origin main

# Vercel auto-deploys!
```

**Step 5: Test Production (5 mins)**
```bash
# Go to your production URL
open https://your-maia-app.vercel.app/conversations

# Test:
# - Search works
# - Filter works
# - Download transcript works
# - Modal opens/closes
```

**Result**: Users can now review, search, and download all their conversations! üéâ

---

## Phase 2: Fix Current Voice Issues (1 hour)

### The Problem We're Solving:
From your console logs this morning:
```
‚ö†Ô∏è WebRTC not connected
‚ùå Falling back to browser TTS (causes echo)
‚ùå Input audio transcription failing
‚ùå States stuck in processing loop
```

### Solution: Proper WebRTC Setup

**Step 1: Create Audio Processor (5 mins)**
```bash
mkdir -p public

cat > public/audio-processor.js << 'EOF'
class AudioProcessor extends AudioWorkletProcessor {
  constructor() {
    super();
    this.bufferSize = 4096;
    this.buffer = new Float32Array(this.bufferSize);
    this.bufferIndex = 0;
  }

  process(inputs, outputs, parameters) {
    const input = inputs[0];
    if (!input || !input[0]) return true;

    const samples = input[0];

    for (let i = 0; i < samples.length; i++) {
      this.buffer[this.bufferIndex++] = samples[i];

      if (this.bufferIndex >= this.bufferSize) {
        this.port.postMessage(this.buffer.slice());
        this.bufferIndex = 0;
      }
    }

    return true;
  }
}

registerProcessor('audio-processor', AudioProcessor);
EOF
```

**Step 2: Fix Backend Ephemeral Token (10 mins)**

Check if this file exists:
```bash
ls app/api/voice/webrtc-session/route.ts
```

If it doesn't exist, create it:
```bash
mkdir -p app/api/voice
```

Let me know if you need me to create this file - it's the key to fixing WebRTC!

**Step 3: Test WebRTC Connection (10 mins)**

After the fix, test:
```bash
npm run dev

# Open browser console
# Start voice conversation
# Look for these logs:
# ‚úÖ "WebRTC handshake complete"
# ‚úÖ "Connected to OpenAI"
# ‚ùå Should NOT see "WebRTC not connected"
```

**Step 4: Verify No Echo Loop (10 mins)**

Test the conversation:
1. Say something to Maya
2. Wait for response
3. Verify she doesn't respond to her own voice
4. Check console - should see:
   - `input_audio_buffer.speech_started` (when YOU speak)
   - `input_audio_buffer.speech_stopped` (when YOU stop)
   - NOT when Maya is speaking

**Result**: WebRTC working, no more echo, reliable voice! ‚úÖ

---

## Phase 3: Add Cost Tracking UI (30 mins)

### What This Does:
Shows users (and you!) exactly how much each conversation costs in real-time.

**Step 1: Create Cost Tracker Component (10 mins)**

```bash
cat > components/CostTracker.tsx << 'EOF'
'use client';

import { useState, useEffect } from 'react';
import { DollarSign, TrendingDown } from 'lucide-react';

interface CostTrackerProps {
  sessionCost: number;
  provider: string;
}

export function CostTracker({ sessionCost, provider }: CostTrackerProps) {
  const baselineCost = 0.12; // OpenAI all-in-one cost for 10-min conversation
  const savings = ((baselineCost - sessionCost) / baselineCost) * 100;

  return (
    <div className="fixed bottom-4 left-4 bg-gradient-to-r from-purple-900 to-pink-900 backdrop-blur-lg text-white px-6 py-4 rounded-xl shadow-2xl border border-white/20">
      <div className="flex items-center gap-4">
        {/* Current Cost */}
        <div>
          <div className="text-xs text-purple-200 mb-1">Session Cost</div>
          <div className="text-3xl font-bold flex items-center gap-1">
            <DollarSign className="w-5 h-5" />
            {sessionCost.toFixed(4)}
          </div>
        </div>

        {/* Savings */}
        {savings > 0 && (
          <div className="border-l border-white/20 pl-4">
            <div className="text-xs text-green-200 mb-1 flex items-center gap-1">
              <TrendingDown className="w-3 h-3" />
              Savings
            </div>
            <div className="text-2xl font-bold text-green-400">
              {savings.toFixed(0)}%
            </div>
          </div>
        )}

        {/* Provider */}
        <div className="border-l border-white/20 pl-4">
          <div className="text-xs text-purple-200 mb-1">Provider</div>
          <div className="text-sm font-medium">{provider}</div>
        </div>
      </div>
    </div>
  );
}

export default CostTracker;
EOF
```

**Step 2: Add to OracleConversation (10 mins)**

In your `components/OracleConversation.tsx`, add:

```typescript
import { CostTracker } from './CostTracker';

// Add state at top
const [sessionCost, setSessionCost] = useState(0);
const [provider, setProvider] = useState('OpenAI');

// In your render/return:
return (
  <div>
    {/* Your existing conversation UI */}

    {/* Add cost tracker */}
    {sessionCost > 0 && (
      <CostTracker
        sessionCost={sessionCost}
        provider={provider}
      />
    )}
  </div>
);
```

**Step 3: Track Costs (10 mins)**

Every time you make an API call, update the cost:

```typescript
// After OpenAI API call
const response = await openai.chat.completions.create({...});

// Estimate cost (rough approximation)
const inputTokens = response.usage?.prompt_tokens || 0;
const outputTokens = response.usage?.completion_tokens || 0;
const cost = (inputTokens * 0.005 / 1000) + (outputTokens * 0.015 / 1000);

setSessionCost(prev => prev + cost);
```

**Result**: Real-time cost tracking visible to users! üí∞

---

## Phase 4: Hybrid SDK (Optional - Save 90%)

### Current Cost Breakdown (10-min conversation):
```
OpenAI Realtime API: $0.12
  - STT: $0.06
  - LLM: $0.03
  - TTS: $0.03
```

### With Hybrid SDK:
```
Local Whisper (STT): $0.00 (FREE!)
Claude (LLM): $0.015 (50% cheaper)
OpenAI TTS (Alloy): $0.03 (same voice you have now)
TOTAL: $0.045 (62% savings!)
```

**OR go full local later:**
```
Local Whisper (STT): $0.00
Claude (LLM): $0.015
Local XTTS (TTS): $0.00 (when you have custom voices)
TOTAL: $0.015 (87.5% savings!)
```

### Deploy Hybrid NOW (1-2 hours):

**Step 1: Start Local Whisper (10 mins)**
```bash
# Start Docker Desktop
open -a Docker

# Start Whisper
docker run -d -p 8001:8001 \
  --name maia-whisper \
  fedirz/faster-whisper-server:latest-cpu

# Test it
curl http://localhost:8001/health
```

**Step 2: Update SDK Config (5 mins)**
```typescript
// In your voice initialization code:
const sdk = new MAIARealtimeSDK({
  providers: {
    whisper: {
      endpoint: 'http://localhost:8001',
      enabled: true  // Use local Whisper
    },
    anthropic: {
      apiKey: process.env.ANTHROPIC_API_KEY,
      enabled: true  // Use Claude for LLM
    },
    openai: {
      apiKey: process.env.OPENAI_API_KEY,
      enabled: true  // Use OpenAI ONLY for TTS (Alloy voice)
    }
  },
  routing: {
    stt: 'whisper',    // Free local
    llm: 'anthropic',  // Cheaper
    tts: 'openai'      // Keep Alloy voice!
  },
  fallbackChain: ['openai']  // If Whisper fails, use OpenAI
});
```

**Step 3: Keep Alloy Voice (5 mins)**

When calling OpenAI TTS, use the voice you already have:
```typescript
const response = await openai.audio.speech.create({
  model: 'tts-1',
  voice: 'alloy',  // Or 'shimmer', 'echo', whatever you're using now
  input: text
});
```

**Result**:
- Same Alloy voice users love
- 62% cost savings
- More reliable (Claude > GPT for conversation)
- Automatic failover to full OpenAI if needed

---

## What Users Experience

### Before (This Morning):
1. User starts conversation
2. Sometimes WebRTC fails ‚Üí echo loop
3. Maya responds (when working)
4. No way to review past conversations
5. No cost visibility
6. Unreliable

### After (With Our Changes):
1. User starts conversation ‚úÖ (more reliable WebRTC)
2. Maya responds ‚úÖ (same voice, better)
3. User sees cost tracker ‚úÖ (bottom left corner)
4. Conversation ends
5. User goes to /conversations ‚úÖ
6. Reviews all past chats ‚úÖ
7. Downloads transcript ‚úÖ
8. Extracts essence ‚úÖ

**Same voice. Better everything else.**

---

## Cost Comparison (Real Numbers)

### Your Current Usage (Estimate):
- 100 conversations/day
- 10 minutes average
- OpenAI all-in: $0.12 √ó 100 = **$12/day = $360/month**

### After Hybrid (Keep Alloy):
- Local Whisper STT: $0
- Claude LLM: $0.015 √ó 100 = $1.50/day
- OpenAI TTS (Alloy): $0.03 √ó 100 = $3/day
- **Total: $4.50/day = $135/month**
- **Savings: $225/month (62%)**

### After Full Local (With Custom Voices Later):
- Local Whisper STT: $0
- Claude LLM: $1.50/day
- Local XTTS: $0
- **Total: $1.50/day = $45/month**
- **Savings: $315/month (87.5%)**

---

## Deployment Order (My Recommendation)

### TODAY (2 hours):

**1. Deploy Conversation UI (30 mins)**
```bash
git add components/EnhancedConversationReview.tsx app/conversations/page.tsx
git commit -m "Add conversation review"
git push
```

**2. Fix WebRTC (1 hour)**
- Create audio-processor.js
- Fix backend token endpoint
- Test voice conversation
- Verify no echo

**3. Add Cost Tracker (30 mins)**
- Create CostTracker component
- Add to OracleConversation
- Test display

### THIS WEEK (2-4 hours):

**4. Deploy Hybrid SDK (Optional)**
- Start local Whisper
- Route STT ‚Üí Whisper
- Route LLM ‚Üí Claude
- Keep TTS ‚Üí OpenAI (Alloy)
- Test failover

### LATER (When You're Ready):

**5. Research Custom Voices**
- Browse voice actors on Fiverr/Voices.com
- Listen to samples
- Decide on Maya/Anthony personalities

**6. Train Custom Voices**
- Hire actors ($800-1200)
- Record phrases (1 week)
- Train XTTS models (automated)
- Deploy custom voices

---

## What To Do RIGHT NOW

**Option A: Quick Win (30 mins)**
Deploy conversation UI right now:
```bash
cd /Users/soullab/MAIA-FRESH
npm run dev
# Test at localhost:3000/conversations
git add .
git commit -m "Add conversation review"
git push
```

**Option B: Fix Voice (1 hour)**
Fix the WebRTC issue from this morning:
1. Create audio-processor.js
2. Fix backend endpoint
3. Test voice
4. Deploy

**Option C: Full Deploy (2-3 hours)**
Do both A + B + add cost tracking

---

## My Recommendation

**Do Option C - Full Deploy:**

1. ‚úÖ Conversation UI (users will love it)
2. ‚úÖ Fix WebRTC (reliability)
3. ‚úÖ Add cost tracker (transparency)
4. ‚è∏Ô∏è Skip custom voices for now (research later)
5. ‚è∏Ô∏è Optional: Add Whisper/Claude hybrid (62% savings)

**Result:**
- Same Alloy voice users know
- Way more reliable
- Beautiful conversation review
- Cost tracking
- Option to save 62% right away
- Option to save 87% when custom voices ready

---

**What do you want to start with?** üöÄ

I recommend starting with the conversation UI (30 mins, big user win), then fixing WebRTC (1 hour, stability win).

**Ready?** Let me know and I'll guide you step-by-step!
