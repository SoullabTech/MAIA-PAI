# Stage 1: Complete Overview

**Last Updated:** 2025-10-01

This document provides the complete roadmap for Stage 1 (Individual Presence) before Stage 2 (Collective Resonance).

---

## Core Principle

> "The journal itself is the foundation. All the metrics, gates, and stages only matter if the basic journaling experience creates genuine value."

**The fundamental test:**
> A user journals once. Three days later, something hard happens. Do they think of MAIA?

If no → the presence loop is broken.
If yes → you have the foundation.

---

## Stage 1 Architecture

### Week 1: Presence Without Performance
**Document:** `/docs/WEEK_1_JOURNAL_FLOW.md`

**Core Loop:**
1. User speaks their truth
2. System receives without rushing
3. Words are preserved
4. Optional: gentle reflection offered
5. User feels heard

**What the journal IS:**
- A witness that breathes with you
- A memory that learns your language
- A mirror that admits its limitations
- A container that preserves without judging

**What the journal is NOT:**
- A therapy bot
- A mood tracker
- A productivity tool
- A social network in waiting

**Week 1 Success:**
- Can someone speak naturally without system anxiety?
- Does saving feel secure and complete?
- Is retrieval simple and reliable?
- Does returning feel welcoming, not surveilled?

---

### Week 2: Repair as Ritual
**Document:** `/docs/WEEK_2_REPAIR_MECHANICS.md`
**Interaction Scripts:** `/docs/WEEK_2_INTERACTION_SCRIPTS.md`

**Design Principle:**
> "If it feels like debugging, the spell is broken."

**Core Features (3 only):**
1. One uncertainty response ("I might be missing something...")
2. Thumbs up/down (post-save, optional, subtle)
3. One correction opportunity ("Want to tell me what I missed?")

**The Hidden Test:**
- Do corrections feel like bonding or babysitting?
- Is correction specificity >70%?
- Does return rate after correction stay ≥85%?

**Week 2 Success:**
- Users correct without feeling like they're training a bot
- Corrections are specific and collaborative
- Repair increases intimacy, not distance

---

### Week 3-8: Iteration and Expansion
**Document:** `/docs/FIRST_8_WEEKS_LAUNCH_GUIDE.md`

**Week 3-4 Candidates (after Week 2 validation):**
- Memory demonstration ("Last time you said...")
- Pattern noticing (without labeling elements)
- Permission seeking ("Want me to remember this differently?")

**Week 5-6:**
- Qualitative interviews (5 users minimum)
- Beta expansion to 50 users
- Pattern stability checks

**Week 7-8:**
- Gate preparation
- Team retrospective
- Evidence collection

**Do NOT proceed to Week 3 until:**
- Correction text is specific 70%+ of the time
- Return rate after correction ≥85%
- No "training a bot" feedback in interviews

---

## 8-Week Operational Cadence

### Weekly Rhythm

**Mondays:**
- Metrics run automatically (`metrics.weekly_scoreboard_v2`)
- Dashboard check: overall status + guardrails
- Product/Eng team 15-min sync: flag anomalies

**Wednesdays:**
- UX research check-in: 3–5 user interviews
- Collect Voice Check qualitative feedback

**Fridays:**
- Team "Therapeutic Health" standup
- Each lead answers: "What's MAIA's therapeutic health this week?"
- Update notes in gate log draft

### Gate Criteria (13 Total)

**Document:** `/docs/STAGE_1_READINESS_CHECKLIST.md`

**Key Metrics:**
1. Repair Engagement & Accuracy: ≥95% accuracy, 0 warnings
2. Trust Signals: ≥70% felt heard
3. Re-Entry Success: ≥60% (7-29d) / ≥45% (≥30d)
4. Escape Hatch: <5% usage
5. Uncertainty Acceptance: >40% by Week 8

**Qualitative Markers:**
- Silence experienced as held space
- Privacy described as viscerally felt
- Corrections feel collaborative
- Vulnerability increases over time

**Operational Markers:**
- Team can answer "What's MAIA's therapeutic health?" in 30 seconds
- Circuit breakers treated with urgency of site outages
- Metrics referenced naturally in sprint planning

---

## Gate Review Process

**Document:** `/docs/stage1_gate_log.md`

**Conducted at Week 8:**
1. Quantitative Review (30 min) - all 13 criteria
2. Qualitative Review (30 min) - interview highlights
3. Retrospective Test (30 min) - 6 canonical questions
4. Gate Decision (30 min) - approve/object/abstain

**Possible Outcomes:**
- ✅ **PASS:** All criteria met, Stage 2 planning begins
- ⚠️ **PARTIAL:** Most criteria met, specific fixes needed, re-review in 4 weeks
- ❌ **FAIL:** Multiple failures, major fixes, reset 8-week clock

**Approval Threshold:**
- 2 of 3 leads approve
- No strong objections (with evidence)

---

## What We're NOT Building (Stage 1)

### Not in Week 1:
- Elemental mapping
- Pattern visualization
- Social features
- Advanced analytics

### Not in Week 2:
- Elemental tagging (too early)
- Agent personality (cognitive load)
- Multiple uncertainty variations (test one first)
- Acting on corrections yet (collect data first)

### Not Before Stage 2:
- Pattern sharing
- Resonance signals
- Spiral partners
- Anonymous connections
- Any interpersonal features

---

## Key Risks to Watch

**Correction Fatigue:**
- If accuracy lags, users may stop correcting (false "success")
- Monitor: correction rate decline, "giving up" signals

**Privacy Theater:**
- If vulnerability doesn't increase, felt safety is missing
- Monitor: entry length, metaphor depth, content sensitivity

**Premature Collectivism:**
- Any push toward Stage 2 features before criteria met
- Monitor: feature requests, team pressure to "add social"

**Metric Drift:**
- If guardrails show WARN/CRIT >2 weeks in a row
- Escalate immediately, don't wait for gate review

---

## Red Flags by Week

### Week 1-2:
- Voice capture failures (transcription broken)
- Breathing animation feels "annoying" or "condescending"
- Escape hatch feels hidden or shameful
- Team arguing about features instead of presence

### Week 3-4:
- Corrections not saving (repair mechanic broken)
- Circuit breaker fires (>5 corrections in one session)
- Users stop correcting ("giving up" pattern)
- Privacy concerns raised

### Week 5-6:
- Pattern instability (elemental mappings shift erratically)
- Re-entry failures (users don't return after 7+ days)
- Trust scores declining
- Team losing focus (feature creep)

### Week 7-8:
- Interviews reveal distrust
- Behavioral markers missing (no vulnerability progression)
- Team retrospective has "no" or "maybe" answers
- Pressure to skip gate review

**If any red flag appears:** Stop. Diagnose. Fix. Document. Don't rush.

---

## Six Retrospective Questions

**Asked at Week 8 gate review:**

1. **Did we measure what mattered?**
   - Are our metrics capturing actual therapeutic integrity?

2. **What surprised us?**
   - What did we expect that didn't happen?
   - What happened that we didn't expect?

3. **What did users teach us about MAIA's voice?**
   - How did the voice evolve from our assumptions?

4. **Where did our assumptions fail?**
   - What hypotheses were wrong?
   - What did we learn from failure?

5. **What would we never compromise again?**
   - What principles emerged as non-negotiable?

6. **Are we ready for collective resonance?**
   - Honest assessment: can users handle Stage 2?

---

## Success Signals (After 8 Weeks)

### Quantitative:
- Correction accuracy trending toward 95%
- Zero circuit breaker incidents in last 4 weeks
- Trust signals ≥70% average
- Re-entry rates meet or exceed targets

### Qualitative:
- Users spontaneously report pattern recognition
- Privacy described as "felt safety" not just policy
- Silence described as "spacious" not "broken"
- Corrections feel like co-creation

### Behavioral:
- Users return after absences without guilt
- Users teach MAIA their language with patience
- Vulnerability increases over time
- Users express curiosity about "others experiencing similar patterns"

### Team:
- Can answer "What's MAIA's therapeutic health?" instantly
- References metrics naturally in sprint planning
- Treats circuit breakers with urgency
- Retrospective questions answered "yes" with conviction

---

## The Most Important Thing

**Keep the journal itself simple and bulletproof.**

Let complexity emerge from user need, not architectural ambition.

The MAIA personality, the Kairos field awareness, the spiral connections - those are all emergent from basic trust.

If the journal doesn't create a sense of being genuinely witnessed in those first few sessions, the rest is architectural fiction.

---

## Document Index

### Core Documents:
- **This Overview:** `/docs/STAGE1_COMPLETE_OVERVIEW.md`
- **Week 1 Flow:** `/docs/WEEK_1_JOURNAL_FLOW.md`
- **Week 2 Mechanics:** `/docs/WEEK_2_REPAIR_MECHANICS.md`
- **Week 2 Scripts:** `/docs/WEEK_2_INTERACTION_SCRIPTS.md`
- **8-Week Guide:** `/docs/FIRST_8_WEEKS_LAUNCH_GUIDE.md`
- **Gate Log:** `/docs/stage1_gate_log.md`

### Supporting Documents:
- **Readiness Checklist:** `/docs/STAGE_1_READINESS_CHECKLIST.md`
- **Metrics System:** `/docs/MAIA_METRICS_SYSTEM_SUMMARY.md`
- **Guardrails:** `/docs/MAIA_GUARDRAILS_RUNBOOK.md`
- **Voice Charter:** `/docs/MAIA_VOICE_CHARTER_v1.1.md`

---

## Next Actions

**Immediate (This Week):**
1. Review this overview with full team (1 hour)
2. Assign Week -3 sprint tasks (presence foundation)
3. Set alpha launch date
4. Draft alpha user invite list (10-20 names)

**Week -3 to -1:**
- Build Week 1 foundation (voice, breathing, save, escape hatch)
- Test internally (does breathing feel alive? does saving feel secure?)
- Don't build Week 2+ yet

**Week 0 (Alpha Launch):**
- Send invites to 10-20 alpha users
- Start daily standups (circuit breaker checks)
- Begin weekly metrics collection

**Week 1-2:**
- Observe baseline behavior
- Collect weekly user surveys
- Resist premature optimization

**Week 3-4:**
- Ship Week 2 features only after Week 1 baseline established
- Begin correction data collection
- Document patterns in gate log

**Week 5-6:**
- Conduct first qualitative interviews
- Expand to 50 users (beta cohort)
- Check pattern stability

**Week 7-8:**
- Prepare gate evidence
- Team retrospective
- Gate review at Week 9

---

## The Clock Starts When the First User Journals

Until then, it's all preparation.

Real users. Real presence. Real metrics.

Everything else is speculation.
