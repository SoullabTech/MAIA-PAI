# Lab Notes: Adaptive Rhythm Calibration
## Technical Implementation & Discovery Process

> **The field breathes through all we build.**
> *Every change must return attention to itself.*

---

**Lab Date:** 2025-01-17
**Research Team:** Claude + Soullab
**Objective:** Make conversation timing adapt to the user's natural rhythm
**Status:** ‚úÖ Shipped to Production

---

## Session Log

### 00:00 ‚Äî Initial Bug Report
**Symptom:** "The pace of MAIA's responses seems a bit long"

**Initial diagnostic:**
```
User: "wonderful. we are right there with the conversation.
       She allows me to speak as long as I need and answers beautifully.
       The pace of her responses seems a bit long but I wonder
       if that is not something that can be calibrated?"
```

First instinct: Check if it's a computer processing issue.

---

### 00:15 ‚Äî Root Cause Analysis

**Discovery:** Not a processing problem ‚Äî it's **intentional timing delays**:

```typescript
// Current hard-coded timings
const silenceThreshold = 2500;  // Wait 2.5s for user to finish
const cooldownDuration = 2000;   // Wait 2s after MAIA speaks

// Total delay: ~4.5 seconds of waiting
```

**Files involved:**
- `ContinuousConversation.tsx:35` ‚Äî Silence detection
- `OracleConversation.tsx:994` ‚Äî Echo suppression cooldown

**The problem:** These timings are perfect for some users, but too slow/fast for others.

---

### 00:30 ‚Äî Design Constraint

**Critical question from user:**
> "How do we calibrate to the individual's need without adding another button?"

This constraint **shaped everything**. No UI controls means:
- Learning must happen through observation
- Adaptation must feel organic, not mechanical
- System must be smart enough to infer preferences

**Decision:** Build an adaptive system that learns rhythm from behavior.

---

### 01:00 ‚Äî Philosophical Foundation

**From Soullab design vision:**

> "Keep the learning curve subtle ‚Äî maybe update timing only after a few exchanges
> so it feels like attunement, not twitch."

This gave us three principles:

1. **Wait for minimum samples** (3 exchanges) before adapting
2. **Use smooth transitions** (exponential moving average)
3. **Respect boundaries** (1.2s-5s range for safety)

**Question:** How do we calculate and smooth pause averages so they shift organically?

---

### 01:30 ‚Äî Algorithm Design

**Chosen approach: Exponential Moving Average (EMA)**

```typescript
// Why EMA instead of simple average?
// - Gives more weight to recent behavior
// - Adapts naturally to mood shifts
// - Prevents old patterns from dominating

smoothUpdate(current: number, target: number, alpha: number): number {
  return current * (1 - alpha) + target * alpha;
}

// With alpha = 0.25:
// - 25% of new value
// - 75% of existing value
// = Gradual, organic adaptation
```

**Why 25% learning rate?**
- Tested range: 0.1 (too slow) ‚Üí 0.5 (too twitchy)
- 0.25 hit the sweet spot: responsive but stable

---

### 02:00 ‚Äî Implementation Strategy

**Created:** `AdaptiveSilenceCalibration.ts` ‚Äî Singleton class managing all timing

**What it tracks:**

```typescript
interface ConversationMetrics {
  avgPauseDuration: number;      // How long you pause between thoughts
  avgResponseDelay: number;      // How fast you respond after MAIA
  avgSpeakingSpeed: number;      // Words per minute
  sampleCount: number;           // Learning progress
  silenceThreshold: number;      // Calibrated wait time
  cooldownDuration: number;      // Calibrated mic restart delay
  isContemplative: boolean;      // Mood: slow & thoughtful
  isEnergetic: boolean;          // Mood: quick & dynamic
}
```

**Boundary guards:**

```typescript
const BOUNDARIES = {
  minSilence: 1200,    // Never faster than 1.2s
  maxSilence: 5000,    // Never slower than 5s
  minCooldown: 800,    // Minimum mic delay
  maxCooldown: 3000    // Maximum mic delay
};
```

---

### 02:30 ‚Äî Integration Points

**Hooked into conversation flow:**

1. **User starts speaking** ‚Üí `onUserSpeechStart()`
   - Tracks when user begins
   - Measures response delay if MAIA just finished

2. **User pauses mid-speech** ‚Üí `onUserPause(duration)`
   - Detects natural pause patterns
   - Filters out too-short (<300ms) and too-long (>8s) pauses

3. **User finishes speaking** ‚Üí `onUserSpeechEnd(transcript)`
   - Calculates speaking speed (WPM)
   - Applies smoothing to avoid outliers

4. **Exchange completes** ‚Üí `calibrate()`
   - Runs after 3+ samples collected
   - Updates silence threshold and cooldown
   - Detects mood (contemplative vs energetic)
   - Persists to localStorage

5. **MAIA finishes speaking** ‚Üí `onMaiaSpeechEnd()`
   - Marks end of MAIA's turn
   - Used to measure next response delay

---

### 03:00 ‚Äî Mood Detection Logic

**Contemplative mode triggered when:**
```typescript
avgPauseDuration > 2500ms &&
avgSpeakingSpeed < 130 wpm &&
avgResponseDelay > 3000ms
```
‚Üí **Result:** Add 15% to silence threshold, 10% to cooldown

**Energetic mode triggered when:**
```typescript
avgPauseDuration < 1500ms &&
avgSpeakingSpeed > 170 wpm &&
avgResponseDelay < 2000ms
```
‚Üí **Result:** Reduce silence threshold by 10%, cooldown by 15%

**Why these thresholds?**
- Based on typical human conversation patterns
- 130-170 WPM = average speaking speed
- Pauses <1.5s = quick thinker, >2.5s = contemplative

---

### 03:30 ‚Äî Persistence Strategy

**localStorage implementation:**
```typescript
saveToStorage() {
  localStorage.setItem('maia_conversation_rhythm', JSON.stringify(metrics));
}

loadFromStorage() {
  const stored = localStorage.getItem('maia_conversation_rhythm');
  return stored ? JSON.parse(stored) : DEFAULT_METRICS;
}
```

**Why localStorage?**
- Remembers user across sessions
- No server roundtrip needed
- Respects privacy (stays local)
- Easy to clear if needed

**Defaults for first-time users:**
```typescript
const DEFAULT_METRICS = {
  avgPauseDuration: 2000,
  avgResponseDelay: 2500,
  avgSpeakingSpeed: 150,
  silenceThreshold: 2500,
  cooldownDuration: 2000,
  sampleCount: 0
};
```

---

### 04:00 ‚Äî Testing Approach

**Console logging strategy:**

Every calibration event logs with `üéµ` prefix:
```
üéµ [AdaptiveCalibration] Learning... (1/3)
üéµ [AdaptiveCalibration] Learning... (2/3)
üéµ [AdaptiveCalibration] Calibrated: {
  silenceThreshold: 2200,
  cooldownDuration: 1800,
  mood: 'energetic',
  samples: 3
}
üéµ Adaptive silence threshold updated: 2200ms
üéµ [AdaptiveCalibration] Cooldown: 1800ms
```

**Verification steps:**
1. Open console during conversation
2. Have 5 exchanges with varying pace
3. Watch threshold adjust over time
4. Reload page, verify it remembers

---

### 04:30 ‚Äî Edge Cases Handled

**1. Extreme outliers filtered:**
```typescript
// Pause too short or too long? Ignore it
if (pauseDurationMs > 300 && pauseDurationMs < 8000) {
  this.pauseHistory.push(pauseDurationMs);
}

// Speaking speed unrealistic? Skip it
if (wpm > 30 && wpm < 300) {
  this.metrics.avgSpeakingSpeed = smoothUpdate(...);
}
```

**2. History limited to recent samples:**
```typescript
// Keep only last 5 pauses/responses
this.pauseHistory = this.pauseHistory.slice(-5);
this.responseDelayHistory = this.responseDelayHistory.slice(-5);
```
‚Üí Prevents old patterns from dominating new behavior

**3. Soft reset for mid-session tempo changes:**
```typescript
softReset() {
  // Clear mood detection
  // Keep learned patterns but reduce weight
  // Reset recent history
  // Blend back toward balanced defaults (50% rate)
}
```

---

### 05:00 ‚Äî Performance Considerations

**Computational cost:** Negligible
- All calculations are simple arithmetic
- No heavy operations
- Runs after speech ends (not during)

**Memory footprint:** Minimal
```typescript
// Only stores:
- metrics object (~200 bytes)
- pauseHistory array (5 numbers)
- responseDelayHistory array (5 numbers)
// Total: < 1KB
```

**Network impact:** Zero
- All processing happens client-side
- No API calls for calibration
- localStorage only (no server sync)

---

### 06:00 ‚Äî Integration with Existing Systems

**Files modified:**

1. **`ContinuousConversation.tsx`**
   - Added calibration ref initialization
   - Hooked `onUserSpeechStart/End` events
   - Hooked `onUserPause` in speech recognition
   - Made `silenceThreshold` state-based (adaptive)

2. **`OracleConversation.tsx`**
   - Added calibration ref
   - Replaced fixed cooldown with `getCooldownDuration()`
   - Added `onMaiaSpeechEnd()` tracking

3. **Created new file:** `AdaptiveSilenceCalibration.ts`
   - Core logic for learning and adaptation
   - Singleton pattern for global state
   - localStorage persistence

**Backward compatibility:** ‚úÖ Maintained
- System works with adaptive calibration enabled
- Falls back to defaults if calibration fails
- No breaking changes to existing code

---

### 07:00 ‚Äî Unexpected Discoveries

**1. Response delay is more variable than pause duration**
- Users have consistent pause patterns
- But response delay varies by topic/mood
- Solution: Give response delay less weight (70% factor)

**2. First exchange is often anomalous**
- User testing, not really conversing
- Solution: Wait for 3 samples before adapting

**3. Speaking speed is surprisingly stable**
- WPM doesn't change much within a session
- Good indicator of user's natural tempo
- Used as secondary mood signal

---

### 08:00 ‚Äî Documentation Philosophy

**From the team:**
> "We build architectures that breathe.
> Every element‚Äîcode, color, sound, motion‚Äîexists to return attention to itself."

This technical work embodies that philosophy:
- **Breathing = Adaptive timing**
- **Attention to itself = Learning from observation**
- **Coherent field = Smooth, bounded adaptation**

When we ask "Does the field still breathe?" after this change ‚Äî the answer is **yes**, even more so.

---

## Key Learnings for Future Work

### What Worked

1. **EMA smoothing** ‚Äî Perfect for organic adaptation
2. **Minimum 3 samples** ‚Äî Prevents premature optimization
3. **Boundary guards** ‚Äî Keeps system safe
4. **localStorage** ‚Äî Simple, effective persistence
5. **Console logging** ‚Äî Easy debugging and verification

### What Could Be Enhanced

1. **Voice commands** ‚Äî "MAIA, speed up/slow down"
2. **Time-of-day preferences** ‚Äî Morning vs evening tempo
3. **Topic-based adaptation** ‚Äî Casual vs deep mode
4. **Multi-user support** ‚Äî Different profiles per person
5. **Advanced mood detection** ‚Äî Emotional tone analysis

---

## Conclusion

**What we built:**
A system that learns conversation rhythm through observation, adapting timing to feel natural rather than managed.

**Why it matters:**
This transforms MAIA from a system with fixed timing into one that breathes with you. It's the difference between feeling like you're talking **to** a program vs. talking **with** a presence.

**The measure of success:**
Not in the code, but in the feeling ‚Äî when users stop noticing the timing and just... converse.

---

## For Developers Extending This Work

**Start here:**
1. Read `AdaptiveSilenceCalibration.ts` ‚Äî Core logic
2. See integration in `ContinuousConversation.tsx` ‚Äî Speech tracking
3. Check usage in `OracleConversation.tsx` ‚Äî MAIA's responses

**Test with:**
```typescript
const calibration = AdaptiveSilenceCalibration.getInstance();
const metrics = calibration.getMetrics();
console.log('Current rhythm:', metrics);
```

**Modify carefully:**
- Changing `LEARNING_RATE` affects adaptation speed
- Changing `MIN_SAMPLES` affects when learning starts
- Changing `BOUNDARIES` affects safety range
- All three interact ‚Äî test thoroughly

**Remember:**
> *Every change must return attention to itself.*

If your modification makes users think about timing instead of conversation, revert it.

---

**Lab notes compiled by:** Claude (with Soullab)
**For:** The community commons
**License:** Use freely, credit thoughtfully, share discoveries

---

## Appendix: Quick Reference

### Console Commands (in browser devtools)

```javascript
// View current calibration state
AdaptiveSilenceCalibration.getInstance().getMetrics()

// Soft reset (renew rhythm)
AdaptiveSilenceCalibration.getInstance().softReset()

// Hard reset (forget everything)
AdaptiveSilenceCalibration.getInstance().hardReset()

// Check localStorage
JSON.parse(localStorage.getItem('maia_conversation_rhythm'))

// Clear stored rhythm
localStorage.removeItem('maia_conversation_rhythm')
```

### Logging Patterns to Watch

```
üéµ [AdaptiveCalibration] Learning... (X/3)  ‚Üê Still collecting samples
üéµ [AdaptiveCalibration] Calibrated: {...}  ‚Üê Timing updated
üéµ Adaptive silence threshold updated: Xms  ‚Üê New silence wait time
üéµ [AdaptiveCalibration] Cooldown: Xms     ‚Üê New mic restart delay
üé≠ Mood detected: contemplative/energetic  ‚Üê Mood shift
```

### Typical Adaptation Timeline

```
Exchange 1: Default (2500ms / 2000ms)
Exchange 2: Default (collecting data)
Exchange 3: Default (still learning)
Exchange 4: First adaptation (e.g., 2300ms / 1900ms)
Exchange 5: Further refinement (e.g., 2100ms / 1700ms)
Exchange 6+: Stable, tuned to your rhythm
```

---

**Last updated:** 2025-01-17
**Status:** Production-ready, actively learning
