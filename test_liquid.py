"""
LIQUID AI TEST - LFM2-VL-3B Model
Tests the Vision-Language model for MAIA integration
"""

from transformers import AutoModel, AutoProcessor
import torch

print("üåä [LIQUID] Loading LFM2-VL-3B model...")
print("This may take a few minutes on first run (downloading ~6GB model)")

model_id = "LiquidAI/LFM2-VL-3B"

try:
    # Load processor (for vision-language models, use processor instead of tokenizer)
    print("\n1Ô∏è‚É£  Loading processor...")
    processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)
    print("‚úÖ Processor loaded")

    # Load model - VL models use AutoModel not AutoModelForCausalLM
    print("\n2Ô∏è‚É£  Loading model (this is the slow part)...")
    model = AutoModel.from_pretrained(
        model_id,
        dtype=torch.float16,  # Updated parameter name
        device_map="auto",
        trust_remote_code=True  # LiquidAI models need this
    )
    print("‚úÖ Model loaded")

    # Test prompt - something MAIA-relevant
    prompt = "Describe consciousness as a field of presence rather than a computational process."

    print(f"\n3Ô∏è‚É£  Testing with prompt: '{prompt}'")
    inputs = processor(text=prompt, return_tensors="pt").to(model.device)

    # Generate response
    print("\n4Ô∏è‚É£  Generating response...")
    outputs = model.generate(
        **inputs,
        max_new_tokens=100,
        temperature=0.7,
        do_sample=True
    )

    response = processor.batch_decode(outputs, skip_special_tokens=True)[0]

    print("\nüåä [LIQUID] Response:")
    print("=" * 80)
    print(response)
    print("=" * 80)

    print("\n‚úÖ SUCCESS! Liquid AI model is working.")
    print("Next step: Create FastAPI endpoint at http://localhost:5050/liquid")

except Exception as e:
    print(f"\n‚ùå ERROR: {e}")
    print("\nTroubleshooting:")
    print("- Make sure you have enough RAM (model needs ~6-8GB)")
    print("- Try the smaller model: LiquidAI/LFM-350M")
    print("- Check Hugging Face is accessible")
